#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ PDF å¤„ç†å™¨
åŠŸèƒ½ï¼š
1. æ‰«æå¹¶å¤„ç† PDF æ–‡ä»¶
2. ä½¿ç”¨ MinerU API è½¬æ¢ä¸º Markdown
3. ä½¿ç”¨ Claude API ç”Ÿæˆæ‘˜è¦
4. çŠ¶æ€è·Ÿè¸ªå’Œæ–­ç‚¹ç»­ä¼ 
"""

import os
import csv
import json
import time
import logging
import io
import tempfile
import zipfile
import threading
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from dotenv import load_dotenv

# å¯¼å…¥æ‘˜è¦ç”Ÿæˆå™¨
from summarizer import AnthropicSummarizer, PaperSummary

load_dotenv()

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    filename: str
    md_converted: bool = False
    summary_generated: bool = False
    md_path: Optional[str] = None
    images_dir: Optional[str] = None
    summary_path: Optional[str] = None
    error_message: Optional[str] = None
    processing_time: float = 0.0
    md_file_reused: bool = False

    def to_dict(self) -> Dict:
        return {
            'filename': self.filename,
            'md_converted': self.md_converted,
            'summary_generated': self.summary_generated,
            'md_path': self.md_path,
            'images_dir': self.images_dir,
            'summary_path': self.summary_path,
            'error_message': self.error_message,
            'processing_time': self.processing_time,
            'md_file_reused': self.md_file_reused
        }


class MinerUClient:
    """MinerU API å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.api_base = "https://mineru.net/api/v4"
        self.headers = {'Authorization': f'Bearer {api_key}'}
        self.max_retries = 4
        self.retry_delay = 5
        self.request_timeout = 180

    def test_connection(self) -> bool:
        """æµ‹è¯• API è¿æ¥"""
        try:
            url = f"{self.api_base}/file-urls/batch"
            data = {
                "enable_formula": False,
                "language": "ch",
                "enable_table": True,
                "files": [{"name": "test.pdf", "is_ocr": True}]
            }
            response = requests.post(url, headers=self.headers, json=data, timeout=30)
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    logging.info("MinerU API è¿æ¥æˆåŠŸ")
                    return True
            logging.warning(f"MinerU API è¿æ¥æµ‹è¯•å¤±è´¥: {response.text[:100]}")
        except Exception as e:
            logging.warning(f"MinerU API è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return False

    def _retry_request(self, func, *args, **kwargs):
        """é‡è¯•æœºåˆ¶"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    wait = self.retry_delay * (2 ** attempt)
                    time.sleep(wait)
        raise last_error

    def get_upload_url(self, filename: str) -> Dict[str, str]:
        """è·å–ä¸Šä¼ é“¾æ¥"""
        def _request():
            url = f"{self.api_base}/file-urls/batch"
            data = {
                "enable_formula": False,
                "language": "ch",
                "enable_table": True,
                "files": [{"name": filename, "is_ocr": True}]
            }
            headers = self.headers.copy()
            headers['Content-Type'] = 'application/json'
            response = requests.post(url, headers=headers, json=data, timeout=self.request_timeout)
            response.raise_for_status()
            result = response.json()
            if result.get('code') != 0:
                raise Exception(f"API é”™è¯¯: {result.get('msg', 'Unknown')}")
            return {
                'batch_id': result['data']['batch_id'],
                'file_url': result['data']['file_urls'][0]
            }
        return self._retry_request(_request)

    def upload_file(self, file_path: Path, upload_url: str) -> bool:
        """ä¸Šä¼ æ–‡ä»¶"""
        def _upload():
            with open(file_path, 'rb') as f:
                response = requests.put(upload_url, data=f, timeout=self.request_timeout)
                response.raise_for_status()
            return True
        return self._retry_request(_upload)

    def get_batch_result(self, batch_id: str) -> Optional[Dict]:
        """è·å–æ‰¹é‡ä»»åŠ¡ç»“æœ"""
        def _request():
            url = f"{self.api_base}/extract-results/batch/{batch_id}"
            response = requests.get(url, headers=self.headers, timeout=self.request_timeout)
            response.raise_for_status()
            result = response.json()
            if result.get('code') != 0:
                raise Exception(f"è·å–ç»“æœå¤±è´¥: {result.get('msg')}")
            return result['data']
        return self._retry_request(_request)

    def wait_for_completion(self, batch_id: str, max_wait: int = 300) -> Optional[str]:
        """ç­‰å¾…å¤„ç†å®Œæˆ"""
        start = time.time()
        while time.time() - start < max_wait:
            try:
                result = self.get_batch_result(batch_id)
                if result and result.get('extract_result'):
                    extract_result = result['extract_result'][0]
                    state = extract_result.get('state')
                    if state == 'done':
                        return extract_result.get('full_zip_url')
                    elif state == 'failed':
                        raise Exception(f"å¤„ç†å¤±è´¥: {extract_result.get('err_msg', 'Unknown')}")
                    elif state in ['pending', 'running', 'converting']:
                        logging.info(f"å¤„ç†çŠ¶æ€: {state}")
            except Exception as e:
                if "å¤„ç†å¤±è´¥" in str(e):
                    raise
                logging.warning(f"æ£€æŸ¥çŠ¶æ€å¤±è´¥: {e}")
            time.sleep(10)
        raise Exception(f"å¤„ç†è¶…æ—¶ï¼ˆ{max_wait}ç§’ï¼‰")

    def download_result(self, download_url: str, output_dir: Path, pdf_name: str) -> Tuple[bool, str, str]:
        """ä¸‹è½½å¤„ç†ç»“æœ"""
        def _download():
            response = requests.get(download_url, timeout=300)
            response.raise_for_status()

            with tempfile.TemporaryDirectory() as temp_dir:
                with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
                    zip_file.extractall(temp_dir)

                # æŸ¥æ‰¾ markdown æ–‡ä»¶
                md_files = []
                for root, dirs, files in os.walk(temp_dir):
                    for f in files:
                        if f.endswith('.md'):
                            md_files.append(os.path.join(root, f))

                if not md_files:
                    raise Exception("ZIP ä¸­æœªæ‰¾åˆ° Markdown æ–‡ä»¶")

                # åˆ›å»ºè¾“å‡ºç›®å½•
                md_dir = output_dir / "md"
                md_dir.mkdir(parents=True, exist_ok=True)

                # å¤åˆ¶ markdown
                pdf_stem = Path(pdf_name).stem
                dst_md = md_dir / f"{pdf_stem}.md"

                with open(md_files[0], 'r', encoding='utf-8') as f:
                    content = f.read()
                with open(dst_md, 'w', encoding='utf-8') as f:
                    f.write(content)

                logging.info(f"Markdown å·²ä¿å­˜: {dst_md}")

                # å¤åˆ¶å›¾ç‰‡
                images_dst = output_dir / "imgs" / pdf_stem
                images_src = None
                for root, dirs, files in os.walk(temp_dir):
                    if 'images' in dirs:
                        images_src = os.path.join(root, 'images')
                        break

                image_count = 0
                if images_src and os.path.exists(images_src):
                    images_dst.mkdir(parents=True, exist_ok=True)
                    for f in os.listdir(images_src):
                        src = os.path.join(images_src, f)
                        dst = images_dst / f
                        if os.path.isfile(src):
                            with open(src, 'rb') as sf:
                                with open(dst, 'wb') as df:
                                    df.write(sf.read())
                            image_count += 1
                    if image_count > 0:
                        logging.info(f"æå– {image_count} ä¸ªå›¾ç‰‡")

                return True, str(dst_md), str(images_dst)

        try:
            return self._retry_request(_download)
        except Exception as e:
            logging.error(f"ä¸‹è½½ç»“æœå¤±è´¥: {e}")
            return False, "", ""

    def convert_pdf(self, pdf_path: Path, output_dir: Path) -> Tuple[bool, str, str]:
        """è½¬æ¢ PDF"""
        try:
            logging.info(f"å¼€å§‹å¤„ç†: {pdf_path.name}")

            if not pdf_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

            size_mb = pdf_path.stat().st_size / (1024 * 1024)
            logging.info(f"æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")

            # è·å–ä¸Šä¼ é“¾æ¥
            upload_info = self.get_upload_url(pdf_path.name)
            batch_id = upload_info['batch_id']
            upload_url = upload_info['file_url']

            # ä¸Šä¼ æ–‡ä»¶
            logging.info("ä¸Šä¼ æ–‡ä»¶ä¸­...")
            self.upload_file(pdf_path, upload_url)

            # ç­‰å¾…å¤„ç†
            logging.info("ç­‰å¾…å¤„ç†å®Œæˆ...")
            download_url = self.wait_for_completion(batch_id)

            if not download_url:
                raise Exception("å¤„ç†å¤±è´¥")

            # ä¸‹è½½ç»“æœ
            logging.info("ä¸‹è½½ç»“æœä¸­...")
            return self.download_result(download_url, output_dir, pdf_path.name)

        except Exception as e:
            logging.error(f"è½¬æ¢å¤±è´¥: {e}")
            return False, "", ""


class PDFProcessor:
    """PDF å¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""

    def __init__(self, config: Dict):
        self.config = config
        self.pdf_dir = Path(config.get('pdf_dir', '01_articles'))
        self.output_dir = Path(config.get('output_dir', '01_articles/processed'))
        self.status_file = Path(config.get('status_file', '.info/.pdf_processing_status.csv'))

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.max_workers = config.get('max_workers', 5)  # é»˜è®¤5ä¸ªå¹¶å‘
        self.enable_parallel = config.get('enable_parallel', True)

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        api_key = config.get('mineru_api_key')
        self.mineru = MinerUClient(api_key) if api_key else None
        self.summarizer = AnthropicSummarizer()

        # åŠ è½½çŠ¶æ€
        self.processing_status = self.load_status()

        # çº¿ç¨‹é”ï¼ˆç”¨äºä¿æŠ¤çŠ¶æ€ä¿å­˜ï¼‰
        self._status_lock = threading.Lock()
        self._print_lock = threading.Lock()

        # è®¾ç½®æ—¥å¿—
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.config.get('log_file', '01_articles/pdf_processing.log')
        log_dir = Path(log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

    def load_status(self) -> Dict[str, Dict]:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        if self.status_file.exists():
            try:
                status = {}
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        filename = row.pop('filename', None)
                        if filename:
                            # è½¬æ¢æ•°æ®ç±»å‹
                            for k, v in row.items():
                                if k in ['md_converted', 'summary_generated', 'md_file_reused']:
                                    row[k] = v.lower() == 'true' if v else False
                                elif k == 'error_message' and v == '':
                                    row[k] = None
                            status[filename] = row
                return status
            except Exception as e:
                logging.warning(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
        return {}

    def save_status(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._status_lock:
            try:
                self.status_file.parent.mkdir(parents=True, exist_ok=True)

                fieldnames = [
                    'filename', 'md_converted', 'summary_generated', 'md_path',
                    'images_dir', 'summary_path', 'error_message',
                    'processing_time', 'md_file_reused'
                ]

                with open(self.status_file, 'w', encoding='utf-8', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()

                    for filename, status in self.processing_status.items():
                        row = {'filename': filename}
                        row.update(status)
                        writer.writerow(row)

            except Exception as e:
                logging.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")

    def get_expected_md_path(self, pdf_name: str) -> Path:
        """è·å–é¢„æœŸçš„ MD æ–‡ä»¶è·¯å¾„"""
        return self.output_dir / "md" / f"{Path(pdf_name).stem}.md"

    def get_expected_summary_path(self, pdf_name: str) -> Path:
        """è·å–é¢„æœŸçš„æ‘˜è¦æ–‡ä»¶è·¯å¾„"""
        return self.output_dir / "summaries" / f"{Path(pdf_name).stem}.json"

    def is_md_file_valid(self, pdf_path: Path) -> Tuple[bool, str]:
        """æ£€æŸ¥ MD æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        md_path = self.get_expected_md_path(pdf_path.name)

        if not md_path.exists():
            return False, "MD æ–‡ä»¶ä¸å­˜åœ¨"

        if md_path.stat().st_size < 100:
            return False, "MD æ–‡ä»¶è¿‡å°"

        pdf_mtime = pdf_path.stat().st_mtime
        md_mtime = md_path.stat().st_mtime
        if md_mtime < pdf_mtime:
            return False, "MD æ–‡ä»¶æ¯” PDF æ—§"

        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                content = f.read()
                if len(content.strip()) < 50:
                    return False, "MD å†…å®¹è¿‡å°‘"
                markdown_indicators = ['#', '##', '**', '![', '|']
                if not any(ind in content for ind in markdown_indicators):
                    return False, "ç¼ºå°‘æœ‰æ•ˆå†…å®¹"
        except Exception as e:
            return False, f"è¯»å–å¤±è´¥: {e}"

        return True, "MD æ–‡ä»¶æœ‰æ•ˆ"

    def is_summary_valid(self, pdf_path: Path) -> Tuple[bool, str]:
        """æ£€æŸ¥æ‘˜è¦æ˜¯å¦æœ‰æ•ˆ"""
        summary_path = self.get_expected_summary_path(pdf_path.name)

        if not summary_path.exists():
            return False, "æ‘˜è¦ä¸å­˜åœ¨"

        # æ£€æŸ¥æ˜¯å¦æ¯” MD æ–‡ä»¶æ–°
        md_path = self.get_expected_md_path(pdf_path.name)
        if md_path.exists():
            md_mtime = md_path.stat().st_mtime
            summary_mtime = summary_path.stat().st_mtime
            if summary_mtime < md_mtime:
                return False, "æ‘˜è¦æ¯” MD æ–‡ä»¶æ—§"

        return True, "æ‘˜è¦æœ‰æ•ˆ"

    def process_single_pdf(self, pdf_path: Path) -> ProcessingResult:
        """å¤„ç†å•ä¸ª PDF"""
        start = time.time()
        result = ProcessingResult(filename=pdf_path.name)

        try:
            logging.info(f"å¼€å§‹å¤„ç†: {pdf_path.name}")

            # 1. æ£€æŸ¥ MD æ–‡ä»¶
            md_valid, md_msg = self.is_md_file_valid(pdf_path)
            md_path = self.get_expected_md_path(pdf_path.name)

            if md_valid:
                logging.info(f"é‡ç”¨ç°æœ‰ MD: {md_msg}")
                result.md_converted = True
                result.md_path = str(md_path)
                result.md_file_reused = True

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆæ‘˜è¦
                summary_valid, _ = self.is_summary_valid(pdf_path)
                if summary_valid and self.summarizer.enabled:
                    result.summary_generated = True
                    result.summary_path = str(self.get_expected_summary_path(pdf_path.name))
            else:
                # 2. è½¬æ¢ PDF
                if self.mineru and self.mineru.test_connection():
                    logging.info("è½¬æ¢ PDF ä¸­...")
                    success, md_p, imgs_dir = self.mineru.convert_pdf(pdf_path, self.output_dir)
                    result.md_converted = success
                    result.md_path = md_p if success else None
                    result.images_dir = imgs_dir if success else None

                    if not success:
                        result.error_message = "PDF è½¬æ¢å¤±è´¥"
                else:
                    result.error_message = "MinerU API ä¸å¯ç”¨"
                    logging.warning(result.error_message)

            # 3. ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if result.md_converted and not result.summary_generated and self.summarizer.enabled:
                try:
                    with open(md_path, 'r', encoding='utf-8') as f:
                        md_content = f.read()

                    summary = self.summarizer.generate_summary(
                        md_content,
                        {'filename': pdf_path.name}
                    )

                    if summary:
                        summary_path = str(self.get_expected_summary_path(pdf_path.name))
                        saved_path = self.summarizer.save_summary(summary, summary_path)
                        if saved_path:
                            result.summary_generated = True
                            result.summary_path = saved_path
                except Exception as e:
                    logging.warning(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

        except Exception as e:
            result.error_message = str(e)
            logging.error(f"å¤„ç†å¼‚å¸¸: {e}")

        finally:
            result.processing_time = time.time() - start

            # ä¿å­˜çŠ¶æ€
            self.processing_status[pdf_path.name] = result.to_dict()
            self.save_status()

        return result

    def process_all_pdfs(self) -> List[ProcessingResult]:
        """å¤„ç†æ‰€æœ‰ PDFï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        if not self.pdf_dir.exists():
            logging.error(f"PDF ç›®å½•ä¸å­˜åœ¨: {self.pdf_dir}")
            return []

        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        total_files = len(pdf_files)

        if total_files == 0:
            logging.info("æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶")
            return []

        logging.info(f"æ‰¾åˆ° {total_files} ä¸ª PDF æ–‡ä»¶")

        # æ ¹æ®é…ç½®é€‰æ‹©ä¸²è¡Œæˆ–å¹¶è¡Œå¤„ç†
        if self.enable_parallel and total_files > 1:
            return self._process_parallel(pdf_files)
        else:
            return self._process_sequential(pdf_files)

    def _process_sequential(self, pdf_files: List[Path]) -> List[ProcessingResult]:
        """ä¸²è¡Œå¤„ç† PDF"""
        results = []
        for pdf_path in pdf_files:
            result = self.process_single_pdf(pdf_path)
            results.append(result)
            self._print_result(result, pdf_path.name)
        return results

    def _process_parallel(self, pdf_files: List[Path]) -> List[ProcessingResult]:
        """å¹¶è¡Œå¤„ç† PDF"""
        results = []
        completed_count = 0
        total_count = len(pdf_files)

        logging.info(f"ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ (å¹¶å‘æ•°: {self.max_workers})")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_pdf = {
                executor.submit(self.process_single_pdf, pdf_path): pdf_path
                for pdf_path in pdf_files
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_pdf):
                pdf_path = future_to_pdf[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed_count += 1

                    # æ‰“å°è¿›åº¦
                    self._print_result(result, pdf_path.name, completed_count, total_count)

                except Exception as e:
                    logging.error(f"å¤„ç† {pdf_path.name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    error_result = ProcessingResult(filename=pdf_path.name)
                    error_result.error_message = str(e)
                    results.append(error_result)
                    self._print_result(error_result, pdf_path.name, completed_count, total_count)

        # æŒ‰æ–‡ä»¶åæ’åºï¼Œä¿æŒä¸€è‡´é¡ºåº
        results.sort(key=lambda r: r.filename)
        return results

    def _print_result(self, result: ProcessingResult, filename: str,
                     current: int = None, total: int = None):
        """æ‰“å°å¤„ç†ç»“æœï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._print_lock:
            status = "âœ…" if result.md_converted else "âŒ"
            summary_mark = " ğŸ“" if result.summary_generated else ""
            reused = " (é‡ç”¨)" if result.md_file_reused else ""

            progress = f"[{current}/{total}] " if current and total else ""
            print(f"{progress}{status} {filename}{reused}{summary_mark} ({result.processing_time:.1f}s)")

            if result.error_message:
                print(f"   âš ï¸  {result.error_message}")

    def generate_report(self, results: List[ProcessingResult]):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        total = len(results)
        if total == 0:
            print("\næ²¡æœ‰ PDF æ–‡ä»¶éœ€è¦å¤„ç†")
            return

        md_converted = sum(1 for r in results if r.md_converted)
        summary_generated = sum(1 for r in results if r.summary_generated)
        md_reused = sum(1 for r in results if r.md_file_reused)
        newly_converted = md_converted - md_reused

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           PDF å¤„ç†æŠ¥å‘Š                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»æ–‡ä»¶æ•°:        {total}
MD è½¬æ¢æˆåŠŸ:     {md_converted} ({md_converted/total*100:.1f}%)
  - æ–°è½¬æ¢:       {newly_converted}
  - é‡ç”¨ç°æœ‰:     {md_reused}
æ‘˜è¦ç”ŸæˆæˆåŠŸ:    {summary_generated}

ğŸ“ å¤„ç†ç»“æœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
MD ç›®å½•:   {self.output_dir}/md/
æ‘˜è¦ç›®å½•: {self.output_dir}/summaries/

ğŸ“„ æ–‡ä»¶è¯¦æƒ…
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        for r in results:
            status = "âœ…" if r.md_converted else "âŒ"
            summary_info = " + ğŸ“" if r.summary_generated else ""
            reused = " (é‡ç”¨)" if r.md_file_reused else ""
            report += f"{status} {r.filename}{reused}{summary_info}\n"

        print(report)


def main():
    """ä¸»å‡½æ•°"""
    config = {
        'pdf_dir': os.getenv('PDF_DIR', '01_articles'),
        'output_dir': os.getenv('OUTPUT_DIR', '01_articles/processed'),
        'status_file': os.getenv('STATUS_FILE', '.info/.pdf_processing_status.csv'),
        'log_file': os.getenv('LOG_FILE', '01_articles/pdf_processing.log'),
        'mineru_api_key': os.getenv('MINERU_API_KEY'),
        # å¹¶è¡Œå¤„ç†é…ç½®
        'max_workers': int(os.getenv('PDF_MAX_WORKERS', '5')),
        'enable_parallel': os.getenv('PDF_ENABLE_PARALLEL', 'true').lower() == 'true',
    }

    print(f"ğŸ“ PDF ç›®å½•: {config['pdf_dir']}")
    print(f"ğŸ”„ å¹¶è¡Œå¤„ç†: {'å¯ç”¨ (å¹¶å‘æ•°: {})'.format(config['max_workers']) if config['enable_parallel'] else 'ç¦ç”¨'}")
    print()

    processor = PDFProcessor(config)
    results = processor.process_all_pdfs()
    processor.generate_report(results)


if __name__ == "__main__":
    main()
